{"cells":[{"cell_type":"markdown","metadata":{"id":"M5LbO0ctx79w"},"source":["## Kaggle Notebook"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","executionInfo":{"elapsed":14654,"status":"ok","timestamp":1690265031898,"user":{"displayName":"손준현","userId":"05342580911122229789"},"user_tz":-540},"id":"Xo5MabJaNlWw","trusted":true},"outputs":[],"source":["\n","# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output.\n","import seaborn as sns\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# pytorch에서 사용할 함수들 호출하기\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"KzvDdzwlNlW8","trusted":true},"outputs":[],"source":["submission = pd.read_csv('./gender_submission.csv')\n","df_train = pd.read_csv('./train.csv')\n","df_test = pd.read_csv('./test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFJLiJ9UNlXB","trusted":true},"outputs":[],"source":["df_train.shape, df_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgT1PlmINlXH","trusted":true},"outputs":[],"source":["dataset =  pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s03vDjidNlXM","trusted":true},"outputs":[],"source":["dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tXY03W7NlXR","trusted":true},"outputs":[],"source":["dataset.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k60aGQkbNlXX","trusted":true},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFQQCSUFNlXa","trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfUOIZiSNlXe","trusted":true},"outputs":[],"source":["df_train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"krmJtwQqNlXh","trusted":true},"outputs":[],"source":["df_test.describe()"]},{"cell_type":"markdown","metadata":{"id":"euTs-aL3NlXl"},"source":["> ### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rhUtt5eNlXm","trusted":true},"outputs":[],"source":["numerical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfTue0W3NlXp","trusted":true},"outputs":[],"source":["dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n","dataset[\"Title\"] = pd.Series(dataset_title)\n","dataset[\"Title\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqKC1-l0NlXs","trusted":true},"outputs":[],"source":["dataset[['Survived', 'Title']].groupby(['Title']).count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1rh8ZhyNlXv","trusted":true},"outputs":[],"source":["class TitleSelector(BaseEstimator, TransformerMixin):\n","    def __init__( self):\n","        self.dict_title = {\n","            \"Capt\":0,\n","            \"Col\":0,\n","            \"Don\":0,\n","            \"Dona\":0,\n","            \"Dr\":4,\n","            \"Jonkheer\":0,\n","            \"Lady\":0,\n","            \"Major\":0,\n","            \"Master\":4,\n","            \"Miss\":2,\n","            \"Mlle\":0,\n","            \"Mme\":0,\n","            \"Mr\":1,\n","            \"Mrs\":3,\n","            \"Ms\":0,\n","            \"Rev\":0,\n","            \"Sir\":0,\n","            \"the Countess\":0\n","        }\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform( self, X, y=None):\n","        for i, name in enumerate(X[\"Name\"]):\n","            for title in self.dict_title.keys():\n","                if title in name:\n","                    X[\"Name\"][i] = self.dict_title[title]\n","                    break\n","\n","            assert X[\"Name\"][i] in self.dict_title.values()\n","\n","        return X\n","\n","name_transformer = Pipeline(steps=[\n","    ('name', TitleSelector()),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ju2TXGVTNlX1","trusted":true},"outputs":[],"source":["dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TL179mEhNlX4","trusted":true},"outputs":[],"source":["g = sns.displot(dataset[\"Fare\"][(dataset[\"Fare\"].notnull())], kde=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJOfztgCNlX7","trusted":true},"outputs":[],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dV3_tedMNlX_","trusted":true},"outputs":[],"source":["dataset = dataset.drop(['Title'], axis=1)\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEP-tZWnNlYC","trusted":true},"outputs":[],"source":["num_cols = [\"Age\", \"Fare\"]\n","cat_cols = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\", \"Embarked\"]\n","cols = num_cols + cat_cols + [\"Name\"]\n","\n","\n","preprocessor = ColumnTransformer(transformers=[\n","    ('num', numerical_transformer, num_cols),\n","    ('name', name_transformer, [\"Name\"]),\n","    ('cat', categorical_transformer, cat_cols),\n","])\n","\n","X_train = preprocessor.fit_transform(df_train[cols])\n","y_train = df_train[\"Survived\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVQoJbLWNlYG","trusted":true},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZHc7gq8NlYI","trusted":true},"outputs":[],"source":["X_test = preprocessor.transform(df_test[cols])\n","X_test.shape"]},{"cell_type":"markdown","metadata":{"id":"J1EL0p-K2Chh"},"source":["### Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2auI5iFn3kYK"},"outputs":[],"source":["#교차 검증\n","X_trn, X_val, y_trn, y_val = train_test_split(X_train, y_train, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vQjjJ-z5qrq"},"outputs":[],"source":["X_trn.toarray().shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ib7kb0Yo9OX7"},"outputs":[],"source":["X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTVFqNzJ2CLI"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, x, y=None):\n","        self.x = torch.from_numpy(x.toarray()).to(torch.float)\n","        if type(y) != type(None):\n","            self.y = torch.from_numpy(y).to(torch.float)\n","        else:\n","            self.y = None\n","\n","    def __getitem__(self, index):\n","        # x = torch.from_numpy(self.x[index]).float()\n","        return self.x[index], self.y[index] if type(self.y) != type(None) else 0\n","\n","    def __len__(self):\n","        return self.x.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNcE8kC2ZZR"},"outputs":[],"source":["train_dataset = CustomDataset(X_trn, y_trn)\n","valid_dataset = CustomDataset(X_val, y_val)\n","test_dataset = CustomDataset(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvMKyiy_2e1K"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"NRNUZjIUNlYP"},"source":["### modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Hl2z_Gs08sB"},"outputs":[],"source":["class TitanicModel(nn.Module):\n","    def __init__(self):\n","        super(TitanicModel, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(856, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(True),\n","            nn.Dropout(0.8),\n","            nn.Linear(64, 32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(True),\n","            nn.Dropout(0.8),\n","            nn.Linear(32, 16),\n","            nn.BatchNorm1d(16),\n","            nn.ReLU(True),\n","            nn.Dropout(0.8),\n","\n","            nn.Linear(16, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        output = self.model(x)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mf6yG5wL1rvh"},"outputs":[],"source":["model = TitanicModel()\n","\n","criterion = nn.BCELoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.02)\n","scheduler = StepLR(optimizer, step_size=20, gamma=0.7)"]},{"cell_type":"markdown","metadata":{"id":"Og3m_Utg35Ya"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIO4P_9O1745"},"outputs":[],"source":["# 모델 학습\n","epochs = 100\n","dry_run = False # 1 배치만 훈련\n","\n","for epoch in range(1, epochs+1):\n","    # 학습\n","    model.train()\n","    train_loss = 0\n","    correct =0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = model(data)\n","        # print(output)\n","        loss = criterion(output, target.view(-1, 1))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.detach().sum()\n","        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    train_loss /= len(train_loader.dataset)\n","    print('Train Epoch: {}/{}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n","        epoch, epochs, train_loss, correct, len(train_loader.dataset),\n","        100. * correct / len(train_loader.dataset)))\n","    if dry_run:\n","        break\n","\n","    # 테스트\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in valid_loader:\n","        with torch.no_grad():\n","            output = model(data)\n","        test_loss += criterion(output, target.view(-1, 1)).detach().sum()  # sum up batch loss\n","        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(valid_loader.dataset)\n","\n","    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(valid_loader.dataset),\n","        100. * correct / len(valid_loader.dataset)))\n","\n","    scheduler.step()"]},{"cell_type":"markdown","metadata":{"id":"oMEEgZNWNlYV"},"source":["### Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3NcBFm05G3m"},"outputs":[],"source":["predictions = []\n","for data, _ in test_loader:\n","    with torch.no_grad():\n","        output = model(data)\n","    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","    predictions += pred.reshape(-1).tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8XQIUyI5UZh"},"outputs":[],"source":["df_pred = pd.DataFrame(df_test[\"PassengerId\"])\n","df_pred[\"Survived\"] = predictions\n","df_pred.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEo3jl4O5WRx"},"outputs":[],"source":["df_pred.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZX9AO30NlYo","trusted":true},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}